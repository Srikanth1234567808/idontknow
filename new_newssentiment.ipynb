{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd08f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (2.4.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: filelock in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bec764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from faiss-cpu) (1.26.3)\n",
      "Requirement already satisfied: packaging in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461664d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chebolu_srikanth/anaconda3/envs/pytorch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b20895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: company_description.csv not found. Please check the path.\n",
      "Creating a dummy DataFrame for demonstration purposes.\n",
      "Using device: cuda\n",
      "Loading BGE model...\n",
      "BGE model loaded.\n",
      "Encoding company descriptions...\n",
      "Company embeddings shape: torch.Size([3, 1024])\n",
      "Building FAISS index...\n",
      "FAISS index built.\n",
      "Loading DeBERTa tokenizer and model for relevance...\n",
      "Error loading DeBERTa model: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "Skipping relevance scoring with DeBERTa.\n",
      "Loading FinBERT tokenizer and model for sentiment...\n",
      "FinBERT model loaded.\n",
      "FinBERT config labels: {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
      "Using FinBERT sentiment labels (ordered by index): ['positive', 'negative', 'neutral']\n",
      "\n",
      "--- Finding relevant companies for news: 'OM Infra Ltd share price surges 8 percent on 129 crore water project win .' ---\n",
      "{'Ticker': 'GOOG', 'RetrievalScore': 0.373, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.718}\n",
      "{'Ticker': 'AAPL', 'RetrievalScore': 0.365, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.718}\n",
      "{'Ticker': 'MSFT', 'RetrievalScore': 0.328, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.718}\n",
      "\n",
      "--- Another example ---\n",
      "\n",
      "--- Finding relevant companies for news: 'A new brand of organic coffee beans is gaining popularity among consumers for its rich flavor and sustainable sourcing.' ---\n",
      "{'Ticker': 'GOOG', 'RetrievalScore': 0.439, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.833}\n",
      "{'Ticker': 'AAPL', 'RetrievalScore': 0.428, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.833}\n",
      "{'Ticker': 'MSFT', 'RetrievalScore': 0.385, 'RelevanceScore_CrossEncoder': 'N/A', 'Sentiment': 'positive', 'SentimentConfidence': 0.833}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "\n",
    "# Ensure sentencepiece is installed: pip install sentencepiece\n",
    "\n",
    "# Load your company data\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")  # Ensure it has 'Ticker' and 'Description'\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: company_description.csv not found. Please check the path.\")\n",
    "    print(\"Creating a dummy DataFrame for demonstration purposes.\")\n",
    "    data = {\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide.',\n",
    "            'Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide.',\n",
    "            'Alphabet Inc. provides online advertising services in the United States, Europe, the Middle East, Africa, the Asia-Pacific, Canada, and Latin America.'\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the BGE encoder for dense retrieval\n",
    "print(\"Loading BGE model...\")\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")  # warmup\n",
    "print(\"BGE model loaded.\")\n",
    "\n",
    "# Encode all company descriptions\n",
    "print(\"Encoding company descriptions...\")\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "print(f\"Company embeddings shape: {company_embeddings.shape}\")\n",
    "\n",
    "# Build FAISS index for retrieval\n",
    "print(\"Building FAISS index...\")\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "# Ensure embeddings are on CPU and numpy for FAISS\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "print(\"FAISS index built.\")\n",
    "\n",
    "# --- Load Cross-Encoders and their specific Tokenizers ---\n",
    "\n",
    "# Relevance Model (DeBERTa)\n",
    "print(\"Loading DeBERTa tokenizer and model for relevance...\")\n",
    "relevance_tokenizer_name = \"microsoft/deberta-v3-large\"\n",
    "# IMPORTANT: If you are using a base DeBERTa model for relevance, its output\n",
    "# logits are NOT inherently relevance scores. It needs to be fine-tuned on a\n",
    "# relevance task (e.g., NLI, or query-document relevance).\n",
    "# For demonstration, we'll assume it's a binary classifier where output[1] is relevance.\n",
    "# If you have a custom fine-tuned model, replace `relevance_model_name`\n",
    "relevance_model_name = \"microsoft/deberta-v3-large\" # Or your fine-tuned relevance model\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(relevance_tokenizer_name)\n",
    "    # If the base model is used, num_labels will be its default (often for MLM or pre-training objectives)\n",
    "    # If you fine-tuned it as a binary classifier for relevance, it would have num_labels=2\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(relevance_model_name) #, num_labels=2 if fine-tuned\n",
    "    relevance_model.to(device)\n",
    "    relevance_model.eval() # Set to evaluation mode\n",
    "    print(\"DeBERTa model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DeBERTa model: {e}\")\n",
    "    print(\"Skipping relevance scoring with DeBERTa.\")\n",
    "    relevance_model = None # Fallback\n",
    "\n",
    "# Sentiment Model (FinBERT)\n",
    "print(\"Loading FinBERT tokenizer and model for sentiment...\")\n",
    "sentiment_tokenizer_name = \"ProsusAI/finbert\"\n",
    "sentiment_model_name = \"ProsusAI/finbert\"\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_tokenizer_name)\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "    sentiment_model.to(device)\n",
    "    sentiment_model.eval() # Set to evaluation mode\n",
    "    print(\"FinBERT model loaded.\")\n",
    "\n",
    "    # Check FinBERT's label mapping\n",
    "    # Typically: {'positive': 0, 'negative': 1, 'neutral': 2} OR {'positive': 2, 'negative': 0, 'neutral': 1} etc.\n",
    "    # The provided code uses [\"negative\", \"neutral\", \"positive\"] which implies negative=0, neutral=1, positive=2\n",
    "    # Let's try to get it from config if possible, otherwise stick to the assumed order\n",
    "    if hasattr(sentiment_model.config, 'id2label'):\n",
    "        # FinBERT from ProsusAI often has labels like: {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "        # We need to map this to your desired output order\n",
    "        config_labels = sentiment_model.config.id2label\n",
    "        print(f\"FinBERT config labels: {config_labels}\")\n",
    "        # Example: if config_labels is {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "        # and you want [\"negative\", \"neutral\", \"positive\"]\n",
    "        # Then index 0 (negative) maps to config's 1, index 1 (neutral) maps to config's 2, index 2 (positive) maps to config's 0.\n",
    "        # This is tricky. For simplicity, we'll use a fixed mapping matching common FinBERT outputs.\n",
    "        # The original code's assumption was: 0 -> negative, 1 -> neutral, 2 -> positive.\n",
    "        # FinBERT (ProsusAI) is typically: positive, negative, neutral.\n",
    "        # So if argmax is 0, it's positive. If 1, negative. If 2, neutral.\n",
    "        # Let's use a mapping for clarity:\n",
    "        finbert_label_map_from_index = {\n",
    "            sentiment_model.config.label2id['positive']: \"positive\",\n",
    "            sentiment_model.config.label2id['negative']: \"negative\",\n",
    "            sentiment_model.config.label2id['neutral']: \"neutral\"\n",
    "        }\n",
    "        sentiment_labels_ordered = [finbert_label_map_from_index[i] for i in range(len(finbert_label_map_from_index))]\n",
    "        print(f\"Using FinBERT sentiment labels (ordered by index): {sentiment_labels_ordered}\")\n",
    "    else:\n",
    "        # Fallback to the original assumption if config is not as expected\n",
    "        sentiment_labels_ordered = [\"negative\", \"neutral\", \"positive\"] # Original assumption\n",
    "        print(f\"Warning: Could not determine FinBERT label order from config. Assuming: {sentiment_labels_ordered}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FinBERT model: {e}\")\n",
    "    print(\"Skipping sentiment scoring.\")\n",
    "    sentiment_model = None # Fallback\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def find_relevant_companies(news_text, top_k=10):\n",
    "    news_embedding = bge_model.encode(news_text, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    # FAISS search needs CPU numpy array\n",
    "    D, indices = faiss_index.search(news_embedding.cpu().unsqueeze(0).numpy(), top_k) # unsqueeze for batch dim\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx == -1: # FAISS can return -1 if fewer than k items are available or for empty results\n",
    "            continue\n",
    "        company_ticker = tickers[idx]\n",
    "        company_desc = descriptions[idx]\n",
    "        retrieval_score = D[0][i] # This is the dot product (IP) score from FAISS\n",
    "\n",
    "        item_result = {\n",
    "            \"Ticker\": company_ticker,\n",
    "            \"RetrievalScore\": round(float(retrieval_score), 3), # Store BGE retrieval score\n",
    "            # \"Description\": company_desc # Optional: for debugging\n",
    "        }\n",
    "\n",
    "        # Relevance scoring with cross-encoder (DeBERTa)\n",
    "        # --- IMPORTANT CAVEAT ---\n",
    "        # The base \"microsoft/deberta-v3-large\" is NOT fine-tuned for relevance.\n",
    "        # Its logits will NOT represent relevance scores without fine-tuning.\n",
    "        # If you have fine-tuned it, it might output 2 classes (relevant, not_relevant).\n",
    "        # The code below assumes the second class (index 1) means \"relevant\".\n",
    "        # If you haven't fine-tuned it, this relevance_score will be meaningless.\n",
    "        if relevance_model and relevance_tokenizer:\n",
    "            inputs = relevance_tokenizer(news_text, company_desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()} # Move inputs to device\n",
    "            with torch.no_grad():\n",
    "                logits = relevance_model(**inputs).logits\n",
    "                # Assuming a binary classifier where output[1] is 'relevant'\n",
    "                # This needs to match how your model was fine-tuned.\n",
    "                # If not fine-tuned, num_labels might be different, and softmax over arbitrary logits is not relevance.\n",
    "                if logits.shape[1] >= 2: # Check if there are at least two output logits\n",
    "                    relevance_prob = torch.softmax(logits, dim=1)[0]\n",
    "                    # Example: if label 1 is \"relevant\"\n",
    "                    # Check model.config.id2label if available for your fine-tuned model\n",
    "                    # For now, let's assume class 1 is 'relevant' if num_labels is 2\n",
    "                    # If num_labels > 2, this is more complex, as it's not a simple binary relevance.\n",
    "                    if relevance_model.config.num_labels == 2:\n",
    "                         # Assuming index 1 is 'relevant', index 0 is 'not relevant'\n",
    "                        relevance_score = relevance_prob[1].item()\n",
    "                    else:\n",
    "                        # Cannot reliably get a binary relevance score from a model not trained for it\n",
    "                        # Or if it has more than 2 classes not clearly mapped to relevance.\n",
    "                        # For demonstration, we'll take the max logit if not binary, but this is NOT a true relevance score.\n",
    "                        relevance_score = relevance_prob.max().item() # This is NOT a proper relevance score\n",
    "                        print(f\"Warning: DeBERTa model does not seem to be a binary relevance classifier (num_labels={relevance_model.config.num_labels}). Relevance score is based on max probability and might not be meaningful.\")\n",
    "\n",
    "                else:\n",
    "                    relevance_score = 0.0 # Cannot determine relevance\n",
    "                    print(f\"Warning: DeBERTa model output logits shape {logits.shape} not suitable for binary relevance.\")\n",
    "\n",
    "            item_result[\"RelevanceScore_CrossEncoder\"] = round(relevance_score, 3)\n",
    "            relevance_threshold = 0.6 # Your threshold\n",
    "        else:\n",
    "            relevance_score = 0.0 # Fallback if model not loaded\n",
    "            relevance_threshold = 0.0 # Effectively disable filtering if no model\n",
    "            item_result[\"RelevanceScore_CrossEncoder\"] = \"N/A\"\n",
    "\n",
    "\n",
    "        # Proceed if deemed relevant by cross-encoder (or if no cross-encoder used)\n",
    "        if not relevance_model or relevance_score > relevance_threshold:\n",
    "            # Sentiment scoring with FinBERT\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                sent_inputs = sentiment_tokenizer(news_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                sent_inputs = {k: v.to(device) for k, v in sent_inputs.items()} # Move inputs to device\n",
    "                with torch.no_grad():\n",
    "                    sent_logits = sentiment_model(**sent_inputs).logits\n",
    "                    sentiment_probabilities = torch.softmax(sent_logits, dim=1)[0]\n",
    "                    predicted_sentiment_index = torch.argmax(sentiment_probabilities).item()\n",
    "                    sentiment_label = sentiment_labels_ordered[predicted_sentiment_index]\n",
    "                    sentiment_confidence = sentiment_probabilities[predicted_sentiment_index].item()\n",
    "\n",
    "                item_result[\"Sentiment\"] = sentiment_label\n",
    "                item_result[\"SentimentConfidence\"] = round(sentiment_confidence, 3)\n",
    "            else:\n",
    "                item_result[\"Sentiment\"] = \"N/A\"\n",
    "                item_result[\"SentimentConfidence\"] = \"N/A\"\n",
    "\n",
    "            results.append(item_result)\n",
    "\n",
    "    # Sort by cross-encoder relevance score if available, otherwise by retrieval score\n",
    "    if relevance_model:\n",
    "        return sorted(results, key=lambda x: -x.get(\"RelevanceScore_CrossEncoder\", 0))\n",
    "    else:\n",
    "        return sorted(results, key=lambda x: -x.get(\"RetrievalScore\", 0))\n",
    "\n",
    "\n",
    "# 🔍 Test\n",
    "if __name__ == \"__main__\":\n",
    "    news_example = \"OM Infra Ltd share price surges 8 percent on 129 crore water project win .\"\n",
    "    print(f\"\\n--- Finding relevant companies for news: '{news_example}' ---\")\n",
    "    relevant_companies = find_relevant_companies(news_example, top_k=5)\n",
    "    if relevant_companies:\n",
    "        for r in relevant_companies:\n",
    "            print(r)\n",
    "    else:\n",
    "        print(\"No relevant companies found.\")\n",
    "\n",
    "    print(\"\\n--- Another example ---\")\n",
    "    news_example_2 = \"A new brand of organic coffee beans is gaining popularity among consumers for its rich flavor and sustainable sourcing.\"\n",
    "    print(f\"\\n--- Finding relevant companies for news: '{news_example_2}' ---\")\n",
    "    relevant_companies_2 = find_relevant_companies(news_example_2, top_k=3)\n",
    "    if relevant_companies_2:\n",
    "        for r in relevant_companies_2:\n",
    "            print(r)\n",
    "    else:\n",
    "        print(\"No relevant companies found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1425b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Article 1: Vodafone Idea went bankrupt.\n",
      "{'Ticker': 'IDEA.NS', 'RetrievalScore': 0.716, 'Sentiment': 'negative'}\n",
      "{'Ticker': 'BHARTIARTL.NS', 'RetrievalScore': 0.553, 'Sentiment': 'negative'}\n",
      "{'Ticker': 'OPTIEMUS.NS', 'RetrievalScore': 0.518, 'Sentiment': 'negative'}\n",
      "\n",
      "Article 2: Adani Power to supply 1,500 MW to Uttar Pradesh\n",
      "{'Ticker': 'ADANIPOWER.NS', 'RetrievalScore': 0.721, 'Sentiment': 'neutral'}\n",
      "{'Ticker': 'ADANIGREEN.NS', 'RetrievalScore': 0.67, 'Sentiment': 'neutral'}\n",
      "{'Ticker': 'RTNPOWER.NS', 'RetrievalScore': 0.625, 'Sentiment': 'neutral'}\n",
      "\n",
      "Article 3: Alphabet’s YouTube expands into podcast streaming.\n",
      "{'Ticker': 'NETWORK18.NS', 'RetrievalScore': 0.46, 'Sentiment': 'neutral'}\n",
      "{'Ticker': 'JUSTDIAL.NS', 'RetrievalScore': 0.443, 'Sentiment': 'neutral'}\n",
      "{'Ticker': 'IMAGICAA.NS', 'RetrievalScore': 0.441, 'Sentiment': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "\n",
    "# === Load Company Data ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load BGE encoder\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "# Encode descriptions\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "# Build FAISS index\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "# Load DeBERTa for relevance scoring\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "# Load FinBERT for sentiment analysis\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Function to Process Articles ===\n",
    "def find_relevant_companies_multiple(articles, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "\n",
    "    article_embeddings = bge_model.encode(articles, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        article_result = {\n",
    "            \"Article\": article,\n",
    "            \"Matches\": []\n",
    "        }\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            # Relevance\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(article, desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            # Sentiment\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            article_result[\"Matches\"].append(result)\n",
    "\n",
    "        results.append(article_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    articles = [\n",
    "        \"Vodafone Idea went bankrupt.\",\n",
    "        \"Adani Power to supply 1,500 MW to Uttar Pradesh\",\n",
    "        \"Alphabet’s YouTube expands into podcast streaming.\"\n",
    "    ]\n",
    "\n",
    "    matches = find_relevant_companies_multiple(articles, top_k=3)\n",
    "    for i, res in enumerate(matches):\n",
    "        print(f\"\\nArticle {i+1}: {res['Article']}\")\n",
    "        for match in res['Matches']:\n",
    "            print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9caa2deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# === Example Usage ===\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Pass the fetched news headlines for analysis\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[43mfind_relevant_companies_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_headlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(matches):\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArticle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m, in \u001b[0;36mfind_relevant_companies_multiple\u001b[0;34m(articles, top_k, relevance_threshold)\u001b[0m\n\u001b[1;32m     82\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m article_embeddings \u001b[38;5;241m=\u001b[39m bge_model\u001b[38;5;241m.\u001b[39mencode(articles, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 85\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, article \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(articles):\n\u001b[1;32m     88\u001b[0m     article_result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m: article,\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatches\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m     91\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/faiss/class_wrappers.py:327\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplacement_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, k, \u001b[38;5;241m*\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, I\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "\n",
    "# === Fetch News from MoneyControl ===\n",
    "url = 'https://www.moneycontrol.com/news/business/'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Look for all divs with class \"clearfix\"\n",
    "articles = soup.find_all('li', class_='clearfix')\n",
    "\n",
    "news_headlines = []\n",
    "for article in articles:\n",
    "    headline_tag = article.find('h2')\n",
    "    if headline_tag:\n",
    "        title = headline_tag.get_text(strip=True)\n",
    "        link_tag = headline_tag.find('a')\n",
    "        link = link_tag['href'] if link_tag else None\n",
    "        news_headlines.append(title)\n",
    "\n",
    "# === Load Company Data ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load BGE encoder\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "# Encode descriptions\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "# Build FAISS index\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "# Load DeBERTa for relevance scoring\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "# Load FinBERT for sentiment analysis\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Function to Process Articles ===\n",
    "def find_relevant_companies_multiple(articles, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "\n",
    "    article_embeddings = bge_model.encode(articles, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        article_result = {\n",
    "            \"Article\": article,\n",
    "            \"Matches\": []\n",
    "        }\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            # Relevance\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(article, desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            # Sentiment\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            article_result[\"Matches\"].append(result)\n",
    "\n",
    "        results.append(article_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Pass the fetched news headlines for analysis\n",
    "    matches = find_relevant_companies_multiple(news_headlines, top_k=3)\n",
    "    for i, res in enumerate(matches):\n",
    "        print(f\"\\nArticle {i+1}: {res['Article']}\")\n",
    "        for match in res['Matches']:\n",
    "            print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05bf8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    131\u001b[0m     news_articles \u001b[38;5;241m=\u001b[39m fetch_moneycontrol_headlines()\n\u001b[0;32m--> 132\u001b[0m     analyzed_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(analyzed_results)\n\u001b[1;32m    135\u001b[0m     df_results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyzed_news.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m, in \u001b[0;36manalyze_articles\u001b[0;34m(articles, top_k, relevance_threshold)\u001b[0m\n\u001b[1;32m     81\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m article_embeddings \u001b[38;5;241m=\u001b[39m bge_model\u001b[38;5;241m.\u001b[39mencode(headlines, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 84\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, headline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(headlines):\n\u001b[1;32m     87\u001b[0m     link \u001b[38;5;241m=\u001b[39m links[i]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/faiss/class_wrappers.py:327\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplacement_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, k, \u001b[38;5;241m*\u001b[39m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, I\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "\n",
    "# === Scrape headlines from Moneycontrol ===\n",
    "def fetch_moneycontrol_headlines():\n",
    "    url = 'https://www.moneycontrol.com/news/business/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='clearfix')\n",
    "\n",
    "    headlines = []\n",
    "    for article in articles:\n",
    "        headline_tag = article.find('h2')\n",
    "        if headline_tag:\n",
    "            title = headline_tag.get_text(strip=True)\n",
    "            link_tag = headline_tag.find('a')\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            headlines.append((title, link))\n",
    "\n",
    "    return headlines\n",
    "\n",
    "# === Load company data ===\n",
    "try:\n",
    "    df = pd.read_csv(\"company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Device and models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "# Load relevance model\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "# Load sentiment model\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Analyze articles ===\n",
    "def analyze_articles(articles, top_k=5, relevance_threshold=0.6):\n",
    "    headlines = [title for title, _ in articles]\n",
    "    links = [link for _, link in articles]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    article_embeddings = bge_model.encode(headlines, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, headline in enumerate(headlines):\n",
    "        link = links[i]\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            # Relevance\n",
    "            relevance_score = None\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(headline, desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            # Sentiment\n",
    "            sentiment = None\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(headline, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "\n",
    "            results.append({\n",
    "                \"Headline\": headline,\n",
    "                \"Link\": link,\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3),\n",
    "                \"RelevanceScore\": round(relevance_score, 3) if relevance_score is not None else None,\n",
    "                \"Sentiment\": sentiment\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    news_articles = fetch_moneycontrol_headlines()\n",
    "    analyzed_results = analyze_articles(news_articles, top_k=5)\n",
    "\n",
    "    df_results = pd.DataFrame(analyzed_results)\n",
    "    df_results.to_csv(\"analyzed_news.csv\", index=False)\n",
    "    print(\"Results saved to analyzed_news.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2203cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results saved to moneycontrol_article_matches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === Load Company Data ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Scrape News from Moneycontrol ===\n",
    "def scrape_moneycontrol_articles():\n",
    "    url = \"https://www.moneycontrol.com/news/business/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    articles = soup.find_all(\"li\", class_=\"clearfix\")\n",
    "\n",
    "    scraped = []\n",
    "    for article in articles:\n",
    "        headline_tag = article.find(\"h2\")\n",
    "        headline = headline_tag.get_text(strip=True) if headline_tag else None\n",
    "\n",
    "        link_tag = article.find(\"a\", href=True)\n",
    "        link = link_tag['href'] if link_tag else None\n",
    "\n",
    "        summary_tag = article.find(\"p\")\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "        if headline:\n",
    "            combined_text = f\"{headline}. {summary}\" if summary else headline\n",
    "            scraped.append({\"headline\": headline, \"summary\": summary, \"link\": link, \"text\": combined_text})\n",
    "\n",
    "    return scraped\n",
    "\n",
    "# === Match Companies ===\n",
    "def find_relevant_companies_multiple(articles, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "    texts = [a[\"text\"] for a in articles]\n",
    "    article_embeddings = bge_model.encode(texts, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        article_result = {\n",
    "            \"Headline\": article[\"headline\"],\n",
    "            \"Summary\": article[\"summary\"],\n",
    "            \"Link\": article[\"link\"],\n",
    "            \"Matches\": []\n",
    "        }\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(article[\"text\"], desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(article[\"text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            article_result[\"Matches\"].append(result)\n",
    "        results.append(article_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Run and Save to CSV ===\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_articles = scrape_moneycontrol_articles()\n",
    "    matches = find_relevant_companies_multiple(scraped_articles, top_k=3)\n",
    "\n",
    "    # Flatten and convert to DataFrame\n",
    "    output_data = []\n",
    "    for article in matches:\n",
    "        if not article[\"Matches\"]:\n",
    "            output_data.append({\n",
    "                \"Headline\": article[\"Headline\"],\n",
    "                \"Summary\": article[\"Summary\"],\n",
    "                \"Link\": article[\"Link\"],\n",
    "                \"Ticker\": None,\n",
    "                \"RetrievalScore\": None,\n",
    "                \"RelevanceScore\": None,\n",
    "                \"Sentiment\": None\n",
    "            })\n",
    "        else:\n",
    "            for match in article[\"Matches\"]:\n",
    "                output_data.append({\n",
    "                    \"Headline\": article[\"Headline\"],\n",
    "                    \"Summary\": article[\"Summary\"],\n",
    "                    \"Link\": article[\"Link\"],\n",
    "                    \"Ticker\": match.get(\"Ticker\"),\n",
    "                    \"RetrievalScore\": match.get(\"RetrievalScore\"),\n",
    "                    \"RelevanceScore\": match.get(\"RelevanceScore\"),\n",
    "                    \"Sentiment\": match.get(\"Sentiment\")\n",
    "                })\n",
    "\n",
    "    df_output = pd.DataFrame(output_data)\n",
    "    df_output.to_csv(\"moneycontrol_article_matches.csv\", index=False)\n",
    "    print(\"Results saved to moneycontrol_article_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b567d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Fact check: Government flags fake X accounts impersonating Vyomika Singh, Sofia Qureshi\n",
      "Link: https://www.moneycontrol.com/news/business/fact-check-government-flags-fake-x-accounts-impersonating-vyomika-singh-sofia-qureshi-13019951.html\n",
      "Summary: The Press Information Bureau’s (PIB) fact-checking unit has clarified that neither Wing Commander Vyomika Singh nor Colonel Sofia Qureshi maintains an official presence on X.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: SP Group launches internal inquiry on arrest of executive in bribery case\n",
      "Link: https://www.moneycontrol.com/news/business/sp-group-launches-internal-inquiry-on-arrest-of-executive-in-bribery-case-13019943.html\n",
      "Summary: The CBI arrested Jeevan Lal Lavidiya, Commissioner of Income Tax (Exemption), Hyderabad, for allegedly accepting a bribe of ₹70 lakh to favour the Shapoorji Pallonji Group.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Adani Power to supply 1,500 MW to Uttar Pradesh\n",
      "Link: https://www.moneycontrol.com/news/business/adani-power-to-supply-1-500-mw-to-uttar-pradesh-13019937.html\n",
      "Summary: This is further to the Uttar Pradesh cabinet approving the project earlier this month, a company statement said.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Govt calls on Bharat Forge, Mahindra to expand production of key defence equipment: Report\n",
      "Link: https://www.moneycontrol.com/news/business/govt-calls-on-bharat-forge-mahindra-to-expand-production-of-key-defence-equipment-report-13019928.html\n",
      "Summary: Private vendors have been directed to enhance supplies of anti-drone and smart ammunition, as well as armoured vehicles capable of integrating loitering munitions and guided missiles.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: CBI arrests Commissioner IT, Shapoorji Pallonji group executive in Rs 70 lakh bribery case\n",
      "Link: https://www.moneycontrol.com/news/business/cbi-arrests-commissioner-it-shapoorji-pallonji-group-executive-in-rs-70-lakh-bribery-case-13019914.html\n",
      "Summary: Lavidiya, a 2004-batch Indian Revenue Service officer, was arrested along with Viral Kantilal Mehta, Deputy General Manager (Taxation) of Shapoorji Pallonji Group; Sairam Palisetty; Natta Veera Naga Sri Ram Gopal; and Sajida Majhar Hussain Shah, they said.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Bombay HC stays Bhool Chuk Maaf's Prime Video premier as PVR Inox takes Maddock to court\n",
      "Link: https://www.moneycontrol.com/entertainment/bombay-hc-stays-bhool-chuk-maaf-s-prime-video-premier-as-pvr-inox-takes-maddock-to-court-article-13019879.html\n",
      "Summary: While the film was supposed to release on May 9, Maddock Films in a tweet on May 8 said that it will take the direct to  streaming route due to the rising tensions between India and Pakistan.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: How To Use ChatGPT To Create a Loan Prepayment Plan: A Step-By-Step Guide\n",
      "Link: https://www.moneycontrol.com/news/business/personal-finance/how-to-use-chatgpt-to-create-a-loan-prepayment-plan-a-step-by-step-guide-13019521.html\n",
      "Summary: Become debt-free faster and save big on interest by planning your loan prepayment. Here's how ChatGPT can help you make a customized plan for yourself.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Delhi airport cancels 60 domestic flights due to Pak escalation after Operation Sindoor: Sources\n",
      "Link: https://www.moneycontrol.com/news/business/60-domestic-flights-cancelled-at-delhi-airport-due-to-pak-escalation-after-operation-sindoor-13019825.html\n",
      "Summary: For now, flight restrictions across north and west India will remain in place till May 15. According to estimates,  around 650 domestic flights were cancelled as a precautionary measure on May 10.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Ixigo suspends flight and hotel bookings for Turkey, Azerbaijan, and China\n",
      "Link: https://www.moneycontrol.com/news/business/ixigo-suspends-flight-and-hotel-bookings-for-turkey-azerbaijan-and-china-13019828.html\n",
      "Summary: Ixigo in a post on X issued a statement saying, \"In solidarity with our nation, we have suspended flight and hotel bookings for Turkey, Azerbaijan, and China.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Swiggy recovers some lost ground in food delivery from Zomato in Q4FY25\n",
      "Link: https://www.moneycontrol.com/news/business/startup/swiggy-recovers-some-lost-ground-in-food-delivery-from-zomato-in-q4fy25-13019804.html\n",
      "Summary: Swiggy's food delivery business saw its adjusted EBITDA margins go up to 2.9 percent in Q4FY25, from 0.5 percent in Q4FY24.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Bomb blast threat to Indore's Holkar Stadium mentions 'Operation Sindoor'; turns out hoax\n",
      "Link: https://www.moneycontrol.com/news/business/bomb-blast-threat-to-indore-s-holkar-stadium-mentions-operation-sindoor-turns-out-hoax-13019795.html\n",
      "Summary: MPCA's (Madhya Pradesh Cricket Association) official email (ID) on Friday received a threatening message. The email, written in English, mentions that the stadium will be blasted due to 'Operation Sindoor' (of Indian armed forces),\" Tukoganj police station in-charge Jitendra Singh Yadav told PTI\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Border tensions snap market's three-week gaining streak despite net FII buying for the week\n",
      "Link: https://www.moneycontrol.com/news/business/markets/border-tensions-snap-market-s-three-week-gaining-streak-despite-net-fii-buying-for-the-week-13019658.html\n",
      "Summary: The benchmark indices ended a three-week gaining streak with a one percent fall, with nearly 40 Nifty 50 names ending in the red on a weekly basis.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: G7 urges India and Pakistan to de-escalate, condemns Pahalgam attack\n",
      "Link: https://www.moneycontrol.com/world/g7-urges-india-and-pakistan-to-de-escalate-condemns-pahalgam-attack-article-13019713.html\n",
      "Summary: The G7 foreign ministers cautioned that any intensification of hostilities could pose a serious threat to regional peace\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Can you get a credit loan with low CIBIL score? Check these key details\n",
      "Link: https://www.moneycontrol.com/news/business/personal-finance/can-you-get-a-credit-loan-with-low-cibil-score-check-these-key-details-13017397.html\n",
      "Summary: Worried about getting a loan with a low cibil score? Here's how to get one with the right strategy and lender.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Market snaps 3-week gaining streak amid Indo-Pak war tensions\n",
      "Link: https://www.moneycontrol.com/news/photos/business/markets/market-snaps-3-week-gaining-streak-amid-indo-pak-war-tensions-13019637.html\n",
      "Summary: The Indian rupee ended lower against the US dollar as domestic currency slipped 81 to end at 85.37 per dollar on May 9 against the May 2 closing of 84.56.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Top 3 trade adjustments, explained by Shubham Agarwal\n",
      "Link: https://www.moneycontrol.com/news/trends/expert-columns/top-3-trade-adjustments-explained-by-shubham-agarwal-13019188.html\n",
      "Summary: Trade Adjustments means modifications done to the original position to accommodate the move that happened after the trade was taken.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: RBI asks banks to remain alert to cyber threats as Pakistan conflict snowballs\n",
      "Link: https://www.moneycontrol.com/news/business/banks/rbi-asks-banks-to-remain-alert-to-cyber-threats-as-pakistan-conflict-snowballs-13019679.html\n",
      "Summary: PSU banks have already started a daily review of branches for cash availability and safety of employees\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Moneycontrol Pro Weekender | The Right Time to Strike Hard: Why India must raise the stakes with Pakistan\n",
      "Link: https://www.moneycontrol.com/news/business/moneycontrol-pro-weekender-the-right-time-to-strike-hard-why-india-must-raise-the-stakes-with-pakistan-13019297.html\n",
      "Summary: What is required is a decisive show of strength — -a big stick to drive home the message that India will no longer tolerate cross-border terrorism\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: IMF hails Pakistan's 'significant progress in stabilising economy' while approving $2.4-billion aid\n",
      "Link: https://www.moneycontrol.com/news/business/imf-hails-pakistan-s-significant-progress-in-stabilising-economy-while-approving-additional-2-4-billion-aid-13019630.html\n",
      "Summary: Risks to the outlook, however, remain elevated, particularly from global economic policy uncertainty, rising geopolitical tensions, and persistent domestic vulnerabilities, the IMF said\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Wall Street closes near flat with focus on US-China talks\n",
      "Link: https://www.moneycontrol.com/news/business/markets/wall-street-closes-near-flat-with-focus-on-us-china-talks-13019636.html\n",
      "Summary: The Dow Jones Industrial Average fell 119.07 points, or 0.29%, to 41,249.38, the S&P 500 lost 4.03 points, or 0.07%, to 5,659.91 and the Nasdaq Composite gained 0.78 points, or flat, to 17,928.92.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Chartist Talks: ICICI Securities' Dharmesh Shah advises not to panic amid Indo-Pak tension-led correction, steam still left in defence pack\n",
      "Link: https://www.moneycontrol.com/news/business/markets/chartist-talks-icici-securities-dharmesh-shah-advises-not-to-panic-amid-indo-pak-tension-led-correction-steam-still-left-in-defence-pack-13019558.html\n",
      "Summary: Looking at the broader bullish structure, Dharmesh Shah of ICICI Securities believes there is still steam left in the defence pack\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Daily Voice: Naveen Kulkarni of Axis Securities believes impact of India-Pakistan tensions on market will be short-lived\n",
      "Link: https://www.moneycontrol.com/news/business/markets/daily-voice-naveen-kulkarni-of-axis-securities-believes-impact-of-india-pakistan-tensions-on-market-will-be-short-lived-13019543.html\n",
      "Summary: A market rally from here will depend on earnings growth and the evolving macroeconomic scenario, which is getting better, said Naveen Kulkarni of Axis Securities.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Govt temporarily shuts over 30 airports amid India-Pakistan tensions; here's the updated list\n",
      "Link: https://www.moneycontrol.com/news/business/govt-temporarily-shuts-over-30-airports-amid-india-pakistan-tensions-here-s-the-updated-list-13019573.html\n",
      "Summary: The Centre also ordered that security across all Indian airports and flights be raised to the highest level with immediate effect.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: Dr Reddy’s projects double-digit FY26 growth, eyes resuming manufacturing across US\n",
      "Link: https://www.moneycontrol.com/news/business/earnings/dr-reddy-s-ceo-projects-double-digit-fy26-growth-with-keen-interest-in-resuming-manufacturing-across-us-13019512.html\n",
      "Summary: CEO Erez Israeli has projected double-digit growth for FY26, primarily on the back of semaglutide launches and pipeline progress.\n",
      "--------------------------------------------------------------------------------\n",
      "Headline: India-New Zealand FTA: Next round of talks in July, aim to conclude negotiations by year-end\n",
      "Link: https://www.moneycontrol.com/news/business/india-new-zealand-fta-next-round-of-talks-in-july-aim-to-conclude-negotiations-by-year-end-13019524.html\n",
      "Summary: \"Constructive negotiations were held across all areas of FTA including Trade in Goods and Services, Trade Facilitation and mutually beneficial sectors of economic co-operation. This engagement highlights the strategic importance both partners attach to building a mutually beneficial, balanced and a fair trade agreement,\" the commerce ministry said in a statement on May 9.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of Moneycontrol's Business News section\n",
    "url = \"https://www.moneycontrol.com/news/business/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all news articles\n",
    "articles = soup.find_all(\"li\", class_=\"clearfix\")\n",
    "\n",
    "# Iterate through the articles and extract information\n",
    "for article in articles:\n",
    "    # Extract the headline\n",
    "    headline_tag = article.find(\"h2\")\n",
    "    headline = headline_tag.get_text(strip=True) if headline_tag else \"No headline\"\n",
    "\n",
    "    # Extract the link\n",
    "    link_tag = article.find(\"a\", href=True)\n",
    "    link = link_tag['href'] if link_tag else \"No link\"\n",
    "\n",
    "    # Extract the summary\n",
    "    summary_tag = article.find(\"p\")\n",
    "    summary = summary_tag.get_text(strip=True) if summary_tag else \"No summary\"\n",
    "\n",
    "    print(f\"Headline: {headline}\")\n",
    "    print(f\"Link: {link}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
