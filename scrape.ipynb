{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9115d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.1\n",
      "    Uninstalling huggingface-hub-0.29.1:\n",
      "      Successfully uninstalled huggingface-hub-0.29.1\n",
      "Successfully installed hf-xet-1.1.0 huggingface-hub-0.31.1 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Using cached faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chebolu_srikanth/anaconda3/envs/tf/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a9146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results saved to moneycontrol_article_matches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === Load Company Data ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Scrape News from Moneycontrol ===\n",
    "def scrape_moneycontrol_articles():\n",
    "    url = \"https://www.moneycontrol.com/news/business/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    articles = soup.find_all(\"li\", class_=\"clearfix\")\n",
    "\n",
    "    scraped = []\n",
    "    for article in articles:\n",
    "        headline_tag = article.find(\"h2\")\n",
    "        headline = headline_tag.get_text(strip=True) if headline_tag else None\n",
    "\n",
    "        link_tag = article.find(\"a\", href=True)\n",
    "        link = link_tag['href'] if link_tag else None\n",
    "\n",
    "        summary_tag = article.find(\"p\")\n",
    "        summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "        if headline:\n",
    "            combined_text = f\"{headline}. {summary}\" if summary else headline\n",
    "            scraped.append({\"headline\": headline, \"summary\": summary, \"link\": link, \"text\": combined_text})\n",
    "\n",
    "    return scraped\n",
    "\n",
    "# === Match Companies ===\n",
    "def find_relevant_companies_multiple(articles, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "    texts = [a[\"text\"] for a in articles]\n",
    "    article_embeddings = bge_model.encode(texts, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        article_result = {\n",
    "            \"Headline\": article[\"headline\"],\n",
    "            \"Summary\": article[\"summary\"],\n",
    "            \"Link\": article[\"link\"],\n",
    "            \"Matches\": []\n",
    "        }\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(article[\"text\"], desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(article[\"text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            article_result[\"Matches\"].append(result)\n",
    "        results.append(article_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Run and Save to CSV ===\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_articles = scrape_moneycontrol_articles()\n",
    "    matches = find_relevant_companies_multiple(scraped_articles, top_k=3)\n",
    "\n",
    "    # Flatten and convert to DataFrame\n",
    "    output_data = []\n",
    "    for article in matches:\n",
    "        if not article[\"Matches\"]:\n",
    "            output_data.append({\n",
    "                \"Headline\": article[\"Headline\"],\n",
    "                \"Summary\": article[\"Summary\"],\n",
    "                \"Link\": article[\"Link\"],\n",
    "                \"Ticker\": None,\n",
    "                \"RetrievalScore\": None,\n",
    "                \"RelevanceScore\": None,\n",
    "                \"Sentiment\": None\n",
    "            })\n",
    "        else:\n",
    "            for match in article[\"Matches\"]:\n",
    "                output_data.append({\n",
    "                    \"Headline\": article[\"Headline\"],\n",
    "                    \"Summary\": article[\"Summary\"],\n",
    "                    \"Link\": article[\"Link\"],\n",
    "                    \"Ticker\": match.get(\"Ticker\"),\n",
    "                    \"RetrievalScore\": match.get(\"RetrievalScore\"),\n",
    "                    \"RelevanceScore\": match.get(\"RelevanceScore\"),\n",
    "                    \"Sentiment\": match.get(\"Sentiment\")\n",
    "                })\n",
    "\n",
    "    df_output = pd.DataFrame(output_data)\n",
    "    df_output.to_csv(\"moneycontrol_article_matches.csv\", index=False)\n",
    "    print(\"Results saved to moneycontrol_article_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed616c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Saved 2500 matched articles to 'matched_moneycontrol_news.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# === Step 1: Load Company Descriptions ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/csv_files/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Step 2: Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "# Load DeBERTa\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "# Load FinBERT\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Step 3: Scrape News Articles ===\n",
    "def scrape_all_moneycontrol_business_news(pages=20, delay=1):\n",
    "    base_url = \"https://www.moneycontrol.com/news/business/page-{}\"\n",
    "    all_articles = []\n",
    "\n",
    "    for page_num in range(1, pages + 1):\n",
    "        url = base_url.format(page_num)\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve page {page_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"li\", class_=\"clearfix\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"No articles found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            headline_tag = article.find(\"h2\")\n",
    "            link_tag = article.find(\"a\", href=True)\n",
    "            summary_tag = article.find(\"p\")\n",
    "\n",
    "            headline = headline_tag.get_text(strip=True) if headline_tag else None\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
    "\n",
    "            if headline:\n",
    "                text = f\"{headline}. {summary}\"\n",
    "                all_articles.append({\n",
    "                    \"Headline\": headline,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Link\": link,\n",
    "                    \"CombinedText\": text\n",
    "                })\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "# === Step 4: Match Articles to Companies ===\n",
    "def find_relevant_companies_multiple(articles_df, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "\n",
    "    article_embeddings = bge_model.encode(\n",
    "        articles_df['CombinedText'].tolist(),\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "        device=device\n",
    "    )\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, row in articles_df.iterrows():\n",
    "        matches = []\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(row['CombinedText'], desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(row['CombinedText'], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            matches.append(result)\n",
    "\n",
    "        results.append({\n",
    "            \"Headline\": row[\"Headline\"],\n",
    "            \"Summary\": row[\"Summary\"],\n",
    "            \"Link\": row[\"Link\"],\n",
    "            \"Matches\": matches\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Step 5: Run Everything ===\n",
    "if __name__ == \"__main__\":\n",
    "    df_articles = scrape_all_moneycontrol_business_news(pages=20)\n",
    "    results = find_relevant_companies_multiple(df_articles)\n",
    "\n",
    "    # Flatten results into rows\n",
    "    output_rows = []\n",
    "    for res in results:\n",
    "        for match in res[\"Matches\"]:\n",
    "            row = {\n",
    "                \"Headline\": res[\"Headline\"],\n",
    "                \"Summary\": res[\"Summary\"],\n",
    "                \"Link\": res[\"Link\"],\n",
    "                **match\n",
    "            }\n",
    "            output_rows.append(row)\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv(\"matched_moneycontrol_news.csv\", index=False)\n",
    "    print(f\"Saved {len(output_df)} matched articles to 'matched_moneycontrol_news.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732c765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     df_articles \u001b[38;5;241m=\u001b[39m scrape_all_moneycontrol_business_news(pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m--> 167\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfind_relevant_companies_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_articles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Flatten results into rows\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     output_rows \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mfind_relevant_companies_multiple\u001b[0;34m(articles_df, top_k, relevance_threshold)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m relevance_score \u001b[38;5;241m<\u001b[39m relevance_threshold:\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentiment_model \u001b[38;5;129;01mand\u001b[39;00m sentiment_tokenizer:\n\u001b[1;32m    144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m sentiment_tokenizer(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedText\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m    145\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:279\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    Size of the full vocabulary with the added tokens.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_added_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# === Step 1: Load Company Descriptions ===\n",
    "try:\n",
    "    df = pd.read_csv(\"/home/chebolu_srikanth/.keras/company_description.csv\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'Ticker': ['AAPL', 'MSFT', 'GOOG'],\n",
    "        'Description': [\n",
    "            'Apple Inc. designs smartphones, computers, and accessories.',\n",
    "            'Microsoft Corp. creates software, services, and devices worldwide.',\n",
    "            'Alphabet Inc. offers online ads and related services globally.'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "tickers = df['Ticker'].tolist()\n",
    "descriptions = df['Description'].tolist()\n",
    "\n",
    "# === Step 2: Setup Device and Models ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bge_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "bge_model.encode(\"warmup\")\n",
    "\n",
    "company_embeddings = bge_model.encode(descriptions, convert_to_tensor=True, normalize_embeddings=True, device=device)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(company_embeddings.shape[1])\n",
    "faiss_index.add(company_embeddings.cpu().numpy())\n",
    "\n",
    "# Load DeBERTa\n",
    "try:\n",
    "    relevance_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\").to(device).eval()\n",
    "except:\n",
    "    relevance_model = None\n",
    "    relevance_tokenizer = None\n",
    "\n",
    "# Load FinBERT\n",
    "try:\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device).eval()\n",
    "    sentiment_labels = sentiment_model.config.id2label\n",
    "except:\n",
    "    sentiment_model = None\n",
    "    sentiment_tokenizer = None\n",
    "    sentiment_labels = None\n",
    "\n",
    "# === Step 3: Scrape News Articles ===\n",
    "def scrape_all_moneycontrol_business_news(pages=20, delay=1):\n",
    "    base_url = \"https://www.moneycontrol.com/news/business/page-{}\"\n",
    "    all_articles = []\n",
    "\n",
    "    for page_num in range(1, pages + 1):\n",
    "        url = base_url.format(page_num)\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve page {page_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"li\", class_=\"clearfix\")\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"No articles found on page {page_num}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            headline_tag = article.find(\"h2\")\n",
    "            link_tag = article.find(\"a\", href=True)\n",
    "            summary_tag = article.find(\"p\")\n",
    "            date_tag = article.find(\"span\", class_=\"dateline\")\n",
    "\n",
    "            headline = headline_tag.get_text(strip=True) if headline_tag else None\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "            summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"\"\n",
    "\n",
    "            if headline:\n",
    "                text = f\"{headline}. {summary}\"\n",
    "                all_articles.append({\n",
    "                    \"Headline\": headline,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Link\": link,\n",
    "                    \"Date\": date,\n",
    "                    \"CombinedText\": text\n",
    "                })\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "# === Step 4: Match Articles to Companies ===\n",
    "def find_relevant_companies_multiple(articles_df, top_k=5, relevance_threshold=0.6):\n",
    "    results = []\n",
    "\n",
    "    article_embeddings = bge_model.encode(\n",
    "        articles_df['CombinedText'].tolist(),\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "        device=device\n",
    "    )\n",
    "    D, I = faiss_index.search(article_embeddings.cpu().numpy(), top_k)\n",
    "\n",
    "    for i, row in articles_df.iterrows():\n",
    "        matches = []\n",
    "\n",
    "        for j, idx in enumerate(I[i]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "\n",
    "            ticker = tickers[idx]\n",
    "            desc = descriptions[idx]\n",
    "            sim_score = float(D[i][j])\n",
    "\n",
    "            result = {\n",
    "                \"Ticker\": ticker,\n",
    "                \"RetrievalScore\": round(sim_score, 3)\n",
    "            }\n",
    "\n",
    "            if relevance_model and relevance_tokenizer:\n",
    "                inputs = relevance_tokenizer(row['CombinedText'], desc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = relevance_model(**inputs).logits\n",
    "                    prob = torch.softmax(logits, dim=1)[0]\n",
    "                    relevance_score = prob[1].item() if relevance_model.config.num_labels == 2 else prob.max().item()\n",
    "                    result[\"RelevanceScore\"] = round(relevance_score, 3)\n",
    "                    if relevance_score < relevance_threshold:\n",
    "                        continue\n",
    "\n",
    "            if sentiment_model and sentiment_tokenizer:\n",
    "                inputs = sentiment_tokenizer(row['CombinedText'], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    logits = sentiment_model(**inputs).logits\n",
    "                    label_id = torch.argmax(logits).item()\n",
    "                    sentiment = sentiment_labels[label_id]\n",
    "                    result[\"Sentiment\"] = sentiment\n",
    "\n",
    "            matches.append(result)\n",
    "\n",
    "        results.append({\n",
    "            \"Headline\": row[\"Headline\"],\n",
    "            \"Summary\": row[\"Summary\"],\n",
    "            \"Link\": row[\"Link\"],\n",
    "            \"Date\": row[\"Date\"],  # Include Date here\n",
    "            \"Matches\": matches\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Step 5: Run Everything ===\n",
    "if __name__ == \"__main__\":\n",
    "    df_articles = scrape_all_moneycontrol_business_news(pages=20)\n",
    "    results = find_relevant_companies_multiple(df_articles)\n",
    "\n",
    "    # Flatten results into rows\n",
    "    output_rows = []\n",
    "    for res in results:\n",
    "        for match in res[\"Matches\"]:\n",
    "            row = {\n",
    "                \"Date\": res[\"Date\"],  # Add Date here\n",
    "                \"Headline\": res[\"Headline\"],\n",
    "                \"Summary\": res[\"Summary\"],\n",
    "                \"Link\": res[\"Link\"],\n",
    "                **match\n",
    "            }\n",
    "            output_rows.append(row)\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv(\"matched_moneycontrol_news.csv\", index=False)\n",
    "    print(f\"Saved {len(output_df)} matched articles to 'matched_moneycontrol_news.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2afbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
